<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basics · HiddenMarkovModels.jl</title><meta name="title" content="Basics · HiddenMarkovModels.jl"/><meta property="og:title" content="Basics · HiddenMarkovModels.jl"/><meta property="twitter:title" content="Basics · HiddenMarkovModels.jl"/><meta name="description" content="Documentation for HiddenMarkovModels.jl."/><meta property="og:description" content="Documentation for HiddenMarkovModels.jl."/><meta property="twitter:description" content="Documentation for HiddenMarkovModels.jl."/><meta property="og:url" content="https://gdalle.github.io/HiddenMarkovModels.jl/examples/basics/"/><meta property="twitter:url" content="https://gdalle.github.io/HiddenMarkovModels.jl/examples/basics/"/><link rel="canonical" href="https://gdalle.github.io/HiddenMarkovModels.jl/examples/basics/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="HiddenMarkovModels.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">HiddenMarkovModels.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Basics</a><ul class="internal"><li><a class="tocitem" href="#Model"><span>Model</span></a></li><li><a class="tocitem" href="#Simulation"><span>Simulation</span></a></li><li><a class="tocitem" href="#Inference"><span>Inference</span></a></li><li><a class="tocitem" href="#Learning"><span>Learning</span></a></li><li><a class="tocitem" href="#Multiple-sequences"><span>Multiple sequences</span></a></li></ul></li><li><a class="tocitem" href="../types/">Types</a></li><li><a class="tocitem" href="../interfaces/">Interfaces</a></li><li><a class="tocitem" href="../temporal/">Time dependency</a></li><li><a class="tocitem" href="../controlled/">Control dependency</a></li><li><a class="tocitem" href="../autodiff/">Autodiff</a></li></ul></li><li><a class="tocitem" href="../../api/">API reference</a></li><li><span class="tocitem">Advanced</span><ul><li><a class="tocitem" href="../../alternatives/">Alternatives</a></li><li><a class="tocitem" href="../../debugging/">Debugging</a></li><li><a class="tocitem" href="../../formulas/">Formulas</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Basics</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basics</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/gdalle/HiddenMarkovModels.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/gdalle/HiddenMarkovModels.jl/blob/main/examples/basics.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Basics"><a class="docs-heading-anchor" href="#Basics">Basics</a><a id="Basics-1"></a><a class="docs-heading-anchor-permalink" href="#Basics" title="Permalink"></a></h1><p>Here we show how to use the essential ingredients of the package.</p><pre><code class="language-julia hljs">using Distributions
using HiddenMarkovModels
using LinearAlgebra
using Random
using StableRNGs</code></pre><pre><code class="language-julia hljs">rng = StableRNG(63);</code></pre><h2 id="Model"><a class="docs-heading-anchor" href="#Model">Model</a><a id="Model-1"></a><a class="docs-heading-anchor-permalink" href="#Model" title="Permalink"></a></h2><p>The package provides a versatile <a href="../../api/#HiddenMarkovModels.HMM"><code>HMM</code></a> type with three main attributes:</p><ul><li>a vector <code>init</code> of state initialization probabilities</li><li>a matrix <code>trans</code> of state transition probabilities</li><li>a vector <code>dists</code> of observation distributions, one for each state</li></ul><p>Any scalar- or vector-valued distribution from <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> can be used for the last part, as well as <a href="../interfaces/#Custom-distributions">Custom distributions</a>.</p><pre><code class="language-julia hljs">init = [0.6, 0.4]
trans = [0.7 0.3; 0.2 0.8]
dists = [MvNormal([-0.5, -0.8], I), MvNormal([0.5, 0.8], I)]
hmm = HMM(init, trans, dists)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Hidden Markov Model with:
 - initialization: [0.6, 0.4]
 - transition matrix: [0.7 0.3; 0.2 0.8]
 - observation distributions: [IsoNormal(
dim: 2
μ: [-0.5, -0.8]
Σ: [1.0 0.0; 0.0 1.0]
)
, IsoNormal(
dim: 2
μ: [0.5, 0.8]
Σ: [1.0 0.0; 0.0 1.0]
)
]</code></pre><h2 id="Simulation"><a class="docs-heading-anchor" href="#Simulation">Simulation</a><a id="Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation" title="Permalink"></a></h2><p>You can simulate a pair of state and observation sequences with <a href="../../api/#Base.rand"><code>rand</code></a> by specifying how long you want them to be.</p><pre><code class="language-julia hljs">T = 20
state_seq, obs_seq = rand(rng, hmm, T);</code></pre><p>The state sequence is a vector of integers.</p><pre><code class="language-julia hljs">state_seq[1:3]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Int64}:
 2
 2
 1</code></pre><p>The observation sequence is a vector whose elements have whatever type an observation distribution returns when sampled. Here we chose a multivariate normal distribution, so we get vectors at each time step.</p><div class="admonition is-warning"><header class="admonition-header">Difference from HMMBase.jl</header><div class="admonition-body"><p>In the case of multivariate observations, HMMBase.jl works with matrices, whereas HiddenMarkovModels.jl works with vectors of vectors. This allows us to accept more generic observations than just numbers or vectors inside the sequence.</p></div></div><pre><code class="language-julia hljs">obs_seq[1:3]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Vector{Float64}}:
 [0.46430432030033214, -0.8538832667367795]
 [0.07113025810845847, 0.35893904118945]
 [-0.8923441428307395, -0.5712530157810823]</code></pre><p>In practical applications, the state sequence is not known, which is why we need inference algorithms to gather information about it.</p><h2 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h2><p>The <strong>Viterbi algorithm</strong> (<a href="../../api/#HiddenMarkovModels.viterbi"><code>viterbi</code></a>) returns:</p><ul><li>the most likely state sequence <span>$\hat{X}_{1:T} = \underset{X_{1:T}}{\mathrm{argmax}}~\mathbb{P}(X_{1:T} \vert Y_{1:T})$</span>,</li><li>the joint loglikelihood <span>$\mathbb{P}(\hat{X}_{1:T}, Y_{1:T})$</span> (in a vector of size 1).</li></ul><pre><code class="language-julia hljs">best_state_seq, best_joint_loglikelihood = viterbi(hmm, obs_seq);
only(best_joint_loglikelihood)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-57.69066363984475</code></pre><p>As we can see, the most likely state sequence is very close to the true state sequence, but not necessarily equal.</p><pre><code class="language-julia hljs">(state_seq .== best_state_seq)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×20 adjoint(::BitVector) with eltype Bool:
 0  0  1  1  1  1  1  1  1  1  1  1  1  0  1  1  1  1  1  1</code></pre><p>The <strong>forward algorithm</strong> (<a href="../../api/#HiddenMarkovModels.forward"><code>forward</code></a>) returns:</p><ul><li>a matrix of filtered state marginals <span>$\alpha[i, t] = \mathbb{P}(X_t = i | Y_{1:t})$</span>,</li><li>the loglikelihood <span>$\mathbb{P}(Y_{1:T})$</span> of the observation sequence (in a vector of size 1).</li></ul><pre><code class="language-julia hljs">filtered_state_marginals, obs_seq_loglikelihood_f = forward(hmm, obs_seq);
only(obs_seq_loglikelihood_f)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-54.214899298272634</code></pre><p>At each time <span>$t$</span>, these filtered marginals take only the observations up to time <span>$t$</span> into account. This is particularly useful to infer the marginal distribution of the last state.</p><pre><code class="language-julia hljs">filtered_state_marginals[:, T]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.7717057517405977
 0.2282942482594023</code></pre><p>The forward-backward algorithm (<a href="../../api/#HiddenMarkovModels.forward_backward"><code>forward_backward</code></a>) returns:</p><ul><li>a matrix of smoothed state marginals <span>$\gamma[i, t] = \mathbb{P}(X_t = i | Y_{1:T})$</span>,</li><li>the loglikelihood <span>$\mathbb{P}(Y_{1:T})$</span> of the observation sequence (in a vector of size 1).</li></ul><pre><code class="language-julia hljs">smoothed_state_marginals, obs_seq_loglikelihood_fb = forward_backward(hmm, obs_seq);
only(obs_seq_loglikelihood_fb)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-54.214899298272634</code></pre><p>At each time <span>$t$</span>, it takes all observations up to time <span>$T$</span> into account. This is particularly useful during learning. Note that forward and forward-backward only coincide at the last time step.</p><pre><code class="language-julia hljs">filtered_state_marginals[:, T - 1] ≈ smoothed_state_marginals[:, T - 1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">false</code></pre><pre><code class="language-julia hljs">filtered_state_marginals[:, T] ≈ smoothed_state_marginals[:, T]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>Finally, we provide a thin wrapper (<a href="../../api/#DensityInterface.logdensityof"><code>logdensityof</code></a>) around the forward algorithm for observation sequence loglikelihoods <span>$\mathbb{P}(Y_{1:T})$</span>.</p><pre><code class="language-julia hljs">logdensityof(hmm, obs_seq)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-54.214899298272634</code></pre><p>Another function (<a href="../../api/#HiddenMarkovModels.joint_logdensityof"><code>joint_logdensityof</code></a>) can compute joint loglikelihoods <span>$\mathbb{P}(X_{1:T}, Y_{1:T})$</span> which take the states into account.</p><pre><code class="language-julia hljs">joint_logdensityof(hmm, obs_seq, state_seq)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-60.170162049564425</code></pre><p>For instance, we can check that the output of Viterbi is at least as likely as the true state sequence.</p><pre><code class="language-julia hljs">joint_logdensityof(hmm, obs_seq, best_state_seq)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">-57.69066363984476</code></pre><h2 id="Learning"><a class="docs-heading-anchor" href="#Learning">Learning</a><a id="Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Learning" title="Permalink"></a></h2><p>The Baum-Welch algorithm (<a href="../../api/#HiddenMarkovModels.baum_welch"><code>baum_welch</code></a>) is a variant of Expectation-Maximization, designed specifically to estimate HMM parameters. Since it is a local optimization procedure, it requires a starting point that is close enough to the true model.</p><pre><code class="language-julia hljs">init_guess = [0.5, 0.5]
trans_guess = [0.6 0.4; 0.3 0.7]
dists_guess = [MvNormal([-0.4, -0.7], I), MvNormal([0.4, 0.7], I)]
hmm_guess = HMM(init_guess, trans_guess, dists_guess);</code></pre><p>Let&#39;s estimate parameters based on a slightly longer sequence.</p><pre><code class="language-julia hljs">_, long_obs_seq = rand(rng, hmm, 200)
hmm_est, loglikelihood_evolution = baum_welch(hmm_guess, long_obs_seq);</code></pre><p>An essential guarantee of this algorithm is that the loglikelihood of the observation sequence keeps increasing as the model improves.</p><pre><code class="language-julia hljs">first(loglikelihood_evolution), last(loglikelihood_evolution)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(-613.0225125416881, -605.3027257918021)</code></pre><p>We can check that the transition matrix estimate has improved.</p><pre><code class="language-julia hljs">cat(transition_matrix(hmm_est), transition_matrix(hmm); dims=3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2×2 Array{Float64, 3}:
[:, :, 1] =
 0.676764  0.323236
 0.197387  0.802613

[:, :, 2] =
 0.7  0.3
 0.2  0.8</code></pre><p>And so have the estimates for the observation distributions.</p><pre><code class="language-julia hljs">map(mean, hcat(obs_distributions(hmm_est), obs_distributions(hmm)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Vector{Float64}}:
 [-0.256414, -0.563875]  [-0.5, -0.8]
 [0.652218, 0.841332]    [0.5, 0.8]</code></pre><p>On the other hand, the initialization is concentrated on one state. This effect can be mitigated by learning from several independent sequences.</p><pre><code class="language-julia hljs">hcat(initialization(hmm_est), initialization(hmm))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
 1.91765e-19  0.6
 1.0          0.4</code></pre><p>Since HMMs are not identifiable up to a permutation of the states, there is no guarantee that state <span>$i$</span> in the true model will correspond to state <span>$i$</span> in the estimated model. This is important to keep in mind when testing new models.</p><h2 id="Multiple-sequences"><a class="docs-heading-anchor" href="#Multiple-sequences">Multiple sequences</a><a id="Multiple-sequences-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-sequences" title="Permalink"></a></h2><p>In many applications, we have access to various observation sequences of different lengths.</p><pre><code class="language-julia hljs">nb_seqs = 100
long_obs_seqs = [last(rand(rng, hmm, rand(rng, 100:200))) for k in 1:nb_seqs];
typeof(long_obs_seqs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Vector{Vector{Vector{Float64}}}<span class="sgr90"> (alias for Array{Array{Array{Float64, 1}, 1}, 1})</span></code></pre><p>Every algorithm in the package accepts multiple sequences in a concatenated form. The user must also specify where each sequence ends in the concatenated vector, by passing <code>seq_ends</code> as a keyword argument. Otherwise, the input will be treated as a unique observation sequence, which is mathematically incorrect.</p><pre><code class="language-julia hljs">long_obs_seq_concat = reduce(vcat, long_obs_seqs)
typeof(long_obs_seq_concat)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Vector{Vector{Float64}}<span class="sgr90"> (alias for Array{Array{Float64, 1}, 1})</span></code></pre><pre><code class="language-julia hljs">seq_ends = cumsum(length.(long_obs_seqs))
seq_ends&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×100 adjoint(::Vector{Int64}) with eltype Int64:
 174  279  388  539  737  894  1007  …  14678  14807  15007  15134  15238</code></pre><p>The outputs of inference algorithms are then concatenated, and the associated loglikelihoods are split by sequence (in a vector of size <code>length(seq_ends)</code>).</p><pre><code class="language-julia hljs">best_state_seq_concat, best_joint_loglikelihood_concat = viterbi(
    hmm, long_obs_seq_concat; seq_ends
);</code></pre><pre><code class="language-julia hljs">length(best_joint_loglikelihood_concat) == length(seq_ends)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><pre><code class="language-julia hljs">length(best_state_seq_concat) == last(seq_ends)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>The function <a href="../../api/#HiddenMarkovModels.seq_limits"><code>seq_limits</code></a> returns the begin and end of a given sequence in the concatenated vector. It can be used to untangle the results.</p><pre><code class="language-julia hljs">start2, stop2 = seq_limits(seq_ends, 2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(175, 279)</code></pre><pre><code class="language-julia hljs">best_state_seq_concat[start2:stop2] == first(viterbi(hmm, long_obs_seqs[2]))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>While inference algorithms can also be run separately on each sequence without changing the results, considering multiple sequences together is nontrivial for Baum-Welch. That is why the package takes care of it automatically.</p><pre><code class="language-julia hljs">hmm_est_concat, _ = baum_welch(hmm_guess, long_obs_seq_concat; seq_ends);</code></pre><p>Our estimate should be a little better.</p><pre><code class="language-julia hljs">cat(transition_matrix(hmm_est_concat), transition_matrix(hmm); dims=3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2×2 Array{Float64, 3}:
[:, :, 1] =
 0.695652  0.304348
 0.207888  0.792112

[:, :, 2] =
 0.7  0.3
 0.2  0.8</code></pre><pre><code class="language-julia hljs">map(mean, hcat(obs_distributions(hmm_est_concat), obs_distributions(hmm)))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Vector{Float64}}:
 [-0.50037, -0.782225]  [-0.5, -0.8]
 [0.519063, 0.80974]    [0.5, 0.8]</code></pre><pre><code class="language-julia hljs">hcat(initialization(hmm_est_concat), initialization(hmm))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
 0.603183  0.6
 0.396817  0.4</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../types/">Types »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 5 April 2024 10:57">Friday 5 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
