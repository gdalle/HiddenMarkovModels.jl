---
title: 'HiddenMarkovModels.jl: generic, fast and reliable latent variable modeling'
tags:
  - Julia
  - statistics
  - hmm
  - inference
  - estimation
authors:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    affiliation: 1
affiliations:
 - name: EPFL (IdePHICS and INDY labs), Switzerland
   index: 1
date: September 2023
bibliography: HMM.bib

---

# Summary

Hidden Markov Models (or HMMs) are a very popular statistical framework, with numerous applications ranging from speech recognition to bioinformatics.
They model a sequence of _observations_ $Y_1, \dots, Y_T$ by assuming the existence of a hidden sequence of _states_ $X_1, \dots, X_T$.
The distribution of a state $X_t$ can only depend on the previous state $X_{t-1}$, and the distribution of an observation $Y_t$ can only depend on the current state $X_t$.
This is a very versatile and practical set of assumptions: see @rabinerTutorialHiddenMarkov1989 for an introduction and @cappeInferenceHiddenMarkov2005 for a book-length treatment.

Given a sequence of observations and a parametric family of HMMs $\mathbb{P}_\theta$, there are several problems one can face.
In generic graphical models, these problems are often intractable, but HMMs have a tree-like structure that yields exact solution procedures with polynomial complexity.

| Problem                                                                                  | Algorithm        |
| ---------------------------------------------------------------------------------------- | ---------------- |
| Observation sequence likelihood $\mathbb{P}_\theta(Y_{1:T})$                             | Forward          |
| State marginals $\mathbb{P}_\theta(X_t \vert Y_{1:T})$                                   | Forward-backward |
| Best state sequence $\mathrm{argmax}_{X_{1:T}}~\mathbb{P}_\theta(X_{1:T} \vert Y_{1:T})$ | Viterbi          |
| Maximum likelihood parameter $\mathrm{argmax}_\theta~\mathbb{P}_\theta(Y_{1:T})$         | Baum-Welch       |

The package `HiddenMarkovModels.jl` leverages the Julia language [@bezansonJuliaFreshApproach2017] to implement those algorithms in a _generic_, _fast_ and _reliable_ way.

# Statement of need

The initial motivation for HiddenMarkovModels.jl was an application of HMMs to reliability analysis for the French railway company SNCF [@dalleMachineLearningCombinatorial2022].
In this industrial use case, the observations were marked temporal point processes (sequences of timed events with structured metadata) generated by condition monitoring systems.

Unfortunately, the major implementations of HMMs we surveyed (in Julia and Python) all expect the observations to be generated by a _predefined set of distributions_.
In Julia, the reference package `HMMBase.jl` [@mouchetHMMBaseJlHidden2023] requires compliance with the `Distributions.jl` [@besanconDistributionsJlDefinition2021] interface, which precludes anything not scalar- or array-valued.
In Python, `hmmlearn` [@hmmlearnHmmlearnHiddenMarkov2023] only offers Gaussians, mixtures of Gaussians and a few discrete distributions (categorical, multinomial, Poisson).
Meanwhile, `pomegranate` [@schreiberPomegranateFastFlexible2018a] has a wider set of available distributions, but it doesn't allow for easy extension by the user.

Focusing on Julia specifically, other downsides of `HMMBase.jl` include the lack of support for multiple observation sequences, and the mandatory use of 64-bit floating point numbers.
Two other packages provide functionalities that `HMMBase.jl` lacks: `HMMGradients.jl` [@antonelloHMMGradientsJlEnables2021] contains a differentiable loglikelihood function, while `MarkovModels.jl` [@ondelGPUAcceleratedForwardBackwardAlgorithm2021] focuses on GPU acceleration.
Unfortunately, all three have mutually incompatible APIs.

# Package design

`HiddenMarkovModels.jl` was designed to overcome the limitations mentioned above, with the following guiding principles in mind.

It is _generic_.
Observations can be arbitrary objects, and the associated distributions only need to implement two methods: a loglikelihood `logdensityof(dist, x)` and a sampler `rand(rng, x)`.
The extendable `AbstractHMM` interface allows incorporating features such as priors or structured transition matrices.
Number types are not restricted, and automatic differentiation of the sequence loglikelihood [@qinDirectOptimizationApproach2000] is supported both in forward and reverse mode.

It is _fast_.
Julia's blend of multiple dispatch and just-in-time compilation delivers satisfactory speed even when working with arbitrary observations.
Inference routines rely on BLAS calls for linear algebra, and exploit multithreading to process sequences in parallel.

It is _reliable_.
The package is thoroughly tested and documented, with an extensive API reference and accessible tutorials.
Special care was given to code quality, type stability, and compatibility checks with various downstream packages.

As a consequence, it is also _limited in scope_.
It centers around CPU efficiency, and remains untested on GPU.
Its primary target is small- to medium-sized HMMs (a few tens of states), mostly because memory requirements scale quadratically for the chosen storage mode.
Finally, it does not perform probability computations in the logarithmic domain, but instead uses the scaling trick from @rabinerTutorialHiddenMarkov1989 with a clever variation borrowed from `HMMBase.jl`.
Thus, its numerical stability might be lower than that of `hmmlearn` or `pomegranate` in challenging instances.
However, thanks to unrestricted number types, users are free to bring in third-party packages like `LogarithmicNumbers.jl` [@rowleyLogarithmicNumbersJlLogarithmic2023] for additional precision.

# Benchmarks

We compare `HiddenMarkovModels.jl` (abbreviated to `HMMs.jl`), `HMMBase.jl`, `hmmlearn` and `pomegranate` on a test case with multivariate Gaussian observations.
The relevant parameters are the number of states `N`, the sequence duration `T`, the observation dimension `D`, the number of sequences `K` and the number of Baum-Welch iterations `I`.
This benchmarking code is run automatically by GitHub Actions upon each package release, but here we ran it on a local Linux system with more cores.
The numbers of Julia, OpenBLAS (for NumPy) and MKL (for PyTorch) threads were all set to 6, although it is hard to compare all libraries fairly when it comes to parallel computing.
See the package documentation for more details on the benchmarks.

In a low-dimensional scenario, `HiddenMarkovModels.jl` runs substantially faster than its predecessor `HMMBase.jl`, even though their algorithms are mathematically identical.
We note that performance is less convincing for the Viterbi algorithm, in which linear operations are replaced by max-plus operations (less easy to optimize).
`pomegranate` is excluded because it does not support multiple sequences.

![](images/low_dim_logdensity_(D=1,T=1000,K=1).svg){ width=50% }
![](images/low_dim_viterbi_(D=1,T=1000,K=1).svg){ width=50% }
![](images/low_dim_forward_backward_(D=1,T=1000,K=1).svg){ width=50% }
![](images/low_dim_baum_welch_(D=1,T=1000,K=1,I=10).svg){ width=50% }

In a high-dimensional scenario, `HiddenMarkovModels.jl` scales favorably compared to both Python alternatives, even though the latter make use of highly optimized C++ backends.
This can partly be explained by the absence of logarithmic computations.
`HMMBase.jl` is excluded because it does not support multiple sequences.

![](images/high_dim_logdensity_(D=10,T=200,K=50).svg){ width=50% }
![](images/high_dim_viterbi_(D=10,T=200,K=50).svg){ width=50% }
![](images/high_dim_forward_backward_(D=10,T=200,K=50).svg){ width=50% }
![](images/high_dim_baum_welch_(D=10,T=200,K=50,I=10).svg){ width=50% }

# Conclusion

`HiddenMarkovModels.jl` fills a longstanding gap in the Julia package ecosystem, and might even prove interesting for Python users.
Future research directions include the implementation of Input-Output HMMs [@bengioInputOutputHMM1994] as well as other estimation methods (gradient-based or spectral).

# Acknowledgements

A special thank you goes to Maxime Mouchet and Jacob Schreiber, the developers of `HMMBase.jl` and `pomegranate` respectively, for their help and advice.
In particular, Maxime agreed to designate `HiddenMarkovModels.jl` as the official successor to `HMMBase.jl`, which I am very grateful for.

# References