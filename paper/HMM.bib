@software{antonelloHMMGradientsJlEnables2021,
  title = {{{HMMGradients}}.Jl: {{Enables}} Computing the Gradient of the Parameters of {{Hidden Markov Models}} ({{HMMs}})},
  shorttitle = {Idiap/{{HMMGradients}}.Jl},
  author = {Antonello, Niccolò},
  date = {2021-06-07},
  doi = {10.5281/zenodo.4454565},
  url = {https://doi.org/10.5281/zenodo.4454565},
  urldate = {2023-09-12},
  organization = {Zenodo},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/PEFYSLF7/4906644.html}
}

@inproceedings{bengioInputOutputHMM1994,
  title = {An {{Input Output HMM Architecture}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Frasconi, Paolo},
  date = {1994},
  volume = {7},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/1994/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
  urldate = {2023-03-12},
  abstract = {We  introduce  a  recurrent  architecture  having  a  modular structure  and we formulate a training procedure based on the EM  algorithm.  The resulting model has similarities to hidden  Markov models, but  supports  recurrent  networks  processing style and allows  to exploit  the supervised  learning paradigm while using maximum likelihood  estimation.},
  keywords = {⛔ No DOI found,hmm,thesis},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/68UNNYP2/Bengio_Frasconi_1994_An Input Output HMM Architecture.pdf}
}

@article{besanconDistributionsJlDefinition2021,
  title = {Distributions{{.jl}}: {{Definition}} and {{Modeling}} of {{Probability Distributions}} in the {{JuliaStats Ecosystem}}},
  shorttitle = {Distributions.Jl},
  author = {Besançon, Mathieu and Papamarkou, Theodore and Anthoff, David and Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  date = {2021-07-25},
  journaltitle = {Journal of Statistical Software},
  volume = {98},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v098.i16},
  url = {https://doi.org/10.18637/jss.v098.i16},
  urldate = {2022-09-19},
  abstract = {Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.},
  langid = {english},
  keywords = {hmm,thesis},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/FZ5V2QNZ/Besancon et al_2021_Distributions.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  date = {2017-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  keywords = {bootstrap,hmm,inferopt,povar,thesis,viva},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@book{cappeInferenceHiddenMarkov2005,
  title = {Inference in {{Hidden Markov Models}}},
  author = {Cappé, Olivier and Moulines, Eric and Rydén, Tobias},
  date = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  eprint = {4d_oEYn8Fl0C},
  eprinttype = {googlebooks},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/0-387-28982-8},
  url = {http://link.springer.com/10.1007/0-387-28982-8},
  urldate = {2022-12-03},
  isbn = {978-0-387-40264-2 978-0-387-28982-3},
  langid = {english},
  keywords = {hmm,povar,thesis},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/2HYZE7ZD/Cappé et al_2005_Inference in Hidden Markov Models.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/QRNV9CL8/Cappé et al. - 2006 - Inference in Hidden Markov Models.pdf}
}

@software{changDynamaxStateSpace2024,
  title = {Dynamax:  {{State Space Models}} Library in {{JAX}}},
  author = {Chang, Peter and Harper-Donnelly, Giles and Kara, Aleyna and Li, Xinglong and Linderman, Scott and Murphy, Kevin},
  date = {2024-02-22T04:10:59Z},
  origdate = {2022-04-11T23:42:29Z},
  url = {https://github.com/probml/dynamax},
  urldate = {2024-02-22},
  abstract = {State Space Models library in JAX},
  organization = {Probabilistic machine learning},
  keywords = {hmm}
}

@online{chenRobustBenchmarkingNoisy2016,
  title = {Robust Benchmarking in Noisy Environments},
  author = {Chen, Jiahao and Revels, Jarrett},
  date = {2016-08-15},
  eprint = {1608.04295},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1608.04295},
  url = {http://arxiv.org/abs/1608.04295},
  urldate = {2024-02-29},
  abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
  pubstate = {preprint},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/E8N8ZJH3/Chen and Revels - 2016 - Robust benchmarking in noisy environments.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/ALUPXBPZ/1608.html}
}

@thesis{dalleMachineLearningCombinatorial2022,
  type = {phdthesis},
  title = {Machine Learning and Combinatorial Optimization Algorithms, with Applications to Railway Planning},
  author = {Dalle, Guillaume},
  date = {2022-12-16},
  institution = {École des Ponts ParisTech},
  url = {https://pastel.hal.science/tel-04053322},
  urldate = {2024-04-05},
  abstract = {This thesis investigates the frontier between machine learning and combinatorial optimization, two active areas of applied mathematics research. We combine theoretical insights with efficient algorithms, and develop several open source Julia libraries. Inspired by a collaboration with the Société nationale des chemins de fer français (SNCF), we study high-impact use cases from the railway world: train failure prediction, delay propagation, and track allocation. In Part I, we provide mathematical background and describe software implementations for various tools that will be needed later on: implicit differentiation, temporal point processes, Hidden Markov Models and Multi-Agent Path Finding. Our publicly available code fills a void in the Julia package ecosystem, aiming at ease of use without compromising on performance. In Part II, we highlight theoretical contributions related to both statistics and decision-making. We consider a Vector AutoRegressive process with partial observations, and prove matching upper and lower bounds on the estimation error. We unify and extend the state of the art for combinatorial optimization layers in deep learning, gathering various approaches in a Julia library called InferOpt.jl. We also seek to differentiate through multi-objective optimization layers, which leads to a novel theory of lexicographic convex analysis. In Part III, these mathematical and algorithmic foundations come together to tackle railway problems. We design a hierarchical model of train failures, propose a graph-based framework for delay propagation, and suggest new avenues for track allocation, with the Flatland challenge as a testing ground.},
  langid = {english},
  keywords = {cv,hmm,povar,website},
  annotation = {HAL\_ID: tel-04053322},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/297KX965/Dalle - 2022 - Machine learning and combinatorial optimization al.pdf}
}

@article{danischMakieJlFlexible2021,
  title = {Makie.Jl: {{Flexible}} High-Performance Data Visualization for {{Julia}}},
  shorttitle = {Makie.Jl},
  author = {Danisch, Simon and Krumbiegel, Julius},
  date = {2021-09-01},
  journaltitle = {Journal of Open Source Software},
  volume = {6},
  number = {65},
  pages = {3349},
  issn = {2475-9066},
  doi = {10.21105/joss.03349},
  url = {https://joss.theoj.org/papers/10.21105/joss.03349},
  urldate = {2024-02-29},
  abstract = {Danisch et al., (2021). Makie.jl: Flexible high-performance data visualization for Julia. Journal of Open Source Software, 6(65), 3349, https://doi.org/10.21105/joss.03349},
  langid = {english},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/T4W5S92V/Danisch and Krumbiegel - 2021 - Makie.jl Flexible high-performance data visualiza.pdf}
}

@software{hmmlearndevelopersHmmlearnHiddenMarkov2023,
  title = {Hmmlearn: {{Hidden Markov Models}} in {{Python}}, with Scikit-Learn like {{API}}},
  author = {{hmmlearn developers}},
  date = {2023},
  url = {https://github.com/hmmlearn/hmmlearn},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models in Python, with scikit-learn like API},
  organization = {hmmlearn},
  keywords = {hmm}
}

@software{mouchetHMMBaseJlHidden2023,
  title = {{{HMMBase}}.Jl:  {{Hidden Markov Models}} for {{Julia}}},
  author = {Mouchet, Maxime},
  date = {2023},
  url = {https://github.com/maxmouchet/HMMBase.jl},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models for Julia.},
  keywords = {hmm}
}

@book{murphyProbabilisticMachineLearning2023,
  title = {Probabilistic {{Machine Learning}}: {{Advanced Topics}}},
  shorttitle = {Probabilistic {{Machine Learning}}},
  author = {Murphy, Kevin P.},
  date = {2023-08-15},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts London, England},
  abstract = {An advanced book for researchers and graduate students working in machine learning and statistics who want to learn about deep learning, Bayesian inference, generative models, and decision making under uncertainty.An advanced counterpart to Probabilistic Machine Learning: An Introduction, this high-level textbook provides researchers and graduate students detailed coverage of cutting-edge topics in machine learning, including deep generative modeling, graphical models, Bayesian inference, reinforcement learning, and causality. This volume puts deep learning into a larger statistical context and unifies approaches based on deep learning with ones based on probabilistic modeling and inference. With contributions from top scientists and domain experts from places such as Google, DeepMind, Amazon, Purdue University, NYU, and the University of Washington, this rigorous book is essential to understanding the vital issues in machine learning.Covers generation of high dimensional outputs, such as images, text, and graphs Discusses methods for discovering insights about data, based on latent variable models Considers training and testing under different distributionsExplores how to use probabilistic models and inference for causal inference and decision makingFeatures online Python code accompaniment},
  isbn = {978-0-262-04843-9},
  langid = {english},
  pagetotal = {1360},
  keywords = {hmm,todo}
}

@inproceedings{ondelGPUAcceleratedForwardBackwardAlgorithm2022,
  title = {{{GPU-Accelerated Forward-Backward Algorithm}} with {{Application}} to {{Lattice-Free MMI}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ondel, Lucas and Lam-Yee-Mui, Léa-Marie and Kocour, Martin and Corro, Caio Filippo and Burget, Lukás},
  date = {2022-05},
  pages = {8417--8421},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746824},
  url = {https://ieeexplore.ieee.org/document/9746824},
  urldate = {2024-03-01},
  abstract = {We propose to express the forward-backward algorithm in terms of operations between sparse matrices in a specific semiring. This new perspective naturally leads to a GPU-friendly algorithm which is easy to implement in Julia or any programming languages with native support of semiring algebra. We use this new implementation to train a TDNN with the LF-MMI objective function and we compare the training time of our system with PyChain—a recently introduced C++/CUDA implementation of the LF-MMI loss. Our implementation is about two times faster while not having to use any approximation such as the "leaky-HMM".},
  eventtitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/PCXZVCW3/Ondel et al. - 2022 - GPU-Accelerated Forward-Backward Algorithm with Ap.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/I5R9E3DD/9746824.html}
}

@article{qinDirectOptimizationApproach2000,
  title = {A {{Direct Optimization Approach}} to {{Hidden Markov Modeling}} for {{Single Channel Kinetics}}},
  author = {Qin, Feng and Auerbach, Anthony and Sachs, Frederick},
  date = {2000-10-01},
  journaltitle = {Biophysical Journal},
  shortjournal = {Biophysical Journal},
  volume = {79},
  number = {4},
  pages = {1915--1927},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(00)76441-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0006349500764411},
  urldate = {2022-08-06},
  abstract = {Hidden Markov modeling (HMM) provides an effective approach for modeling single channel kinetics. Standard HMM is based on Baum's reestimation. As applied to single channel currents, the algorithm has the inability to optimize the rate constants directly. We present here an alternative approach by considering the problem as a general optimization problem. The quasi-Newton method is used for searching the likelihood surface. The analytical derivatives of the likelihood function are derived, thereby maximizing the efficiency of the optimization. Because the rate constants are optimized directly, the approach has advantages such as the allowance for model constraints and the ability to simultaneously fit multiple data sets obtained at different experimental conditions. Numerical examples are presented to illustrate the performance of the algorithm. Comparisons with Baum's reestimation suggest that the approach has a superior convergence speed when the likelihood surface is poorly defined due to, for example, a low signal-to-noise ratio or the aggregation of multiple states having identical conductances.},
  langid = {english},
  keywords = {hmm,thesis,viva},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/EPDNRHUX/Qin et al. - 2000 - A Direct Optimization Approach to Hidden Markov Mo.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/6C5WNKEU/S0006349500764411.html}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  date = {1989-02},
  journaltitle = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {done,hmm,thesis,viva},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/A68ILRMJ/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/BEJEKP4E/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/5BHQF7ME/18626.html}
}

@software{rowleyLogarithmicNumbersJlLogarithmic2023,
  title = {{{LogarithmicNumbers}}.Jl: {{A}} Logarithmic Number System for {{Julia}}.},
  author = {Rowley, Christopher},
  date = {2023-05-24T15:06:29Z},
  url = {https://github.com/cjdoris/LogarithmicNumbers.jl},
  urldate = {2023-09-12},
  abstract = {A logarithmic number system for Julia.},
  keywords = {hmm}
}

@software{rowleyPythonCallJlPython2022,
  title = {{{PythonCall}}.Jl: {{Python}} and {{Julia}} in Harmony},
  author = {Rowley, Christopher},
  date = {2022},
  url = {https://github.com/JuliaPy/PythonCall.jl},
  urldate = {2024-02-29},
  abstract = {Python and Julia in harmony.},
  organization = {JuliaPy},
  keywords = {hmm}
}

@book{sarkkaBayesianFilteringSmoothing2023,
  title = {Bayesian {{Filtering}} and {{Smoothing}}},
  author = {Särkkä, Simo and Svensson, Lennart},
  date = {2023},
  series = {Institute of {{Mathematical Statistics Textbooks}}},
  edition = {2},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781108917407},
  url = {https://www.cambridge.org/core/books/bayesian-filtering-and-smoothing/F88740E8D25010CF3119A5CA379FA37A},
  urldate = {2024-04-05},
  abstract = {Now in its second edition, this accessible text presents a unified Bayesian treatment of state-of-the-art filtering, smoothing, and parameter estimation algorithms for non-linear state space models. The book focuses on discrete-time state space models and carefully introduces fundamental aspects related to optimal filtering and smoothing. In particular, it covers a range of efficient non-linear Gaussian filtering and smoothing algorithms, as well as Monte Carlo-based algorithms. This updated edition features new chapters on constructing state space models of practical systems, the discretization of continuous-time state space models, Gaussian filtering by enabling approximations, posterior linearization filtering, and the corresponding smoothers. Coverage of key topics is expanded, including extended Kalman filtering and smoothing, and parameter estimation. The book's practical, algorithmic approach assumes only modest mathematical prerequisites, suitable for graduate and advanced undergraduate students. Many examples are included, with Matlab and Python code available online, enabling readers to implement algorithms in their own projects.},
  isbn = {978-1-108-92664-5},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/UIIU5GP5/Särkkä and Svensson - 2023 - Bayesian Filtering and Smoothing.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/P3UV29XL/F88740E8D25010CF3119A5CA379FA37A.html}
}

@software{schreiberJmschreiPomegranate2024,
  title = {Jmschrei/Pomegranate},
  author = {Schreiber, Jacob},
  date = {2024-04-03T07:51:56Z},
  origdate = {2014-11-24T18:36:58Z},
  url = {https://github.com/jmschrei/pomegranate},
  urldate = {2024-04-05},
  abstract = {Fast, flexible and easy to use probabilistic modelling in Python.},
  keywords = {hmm}
}

@article{schreiberPomegranateFastFlexible2018,
  title = {Pomegranate: {{Fast}} and {{Flexible Probabilistic Modeling}} in {{Python}}},
  shorttitle = {Pomegranate},
  author = {Schreiber, Jacob},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  number = {164},
  pages = {1--6},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-636.html},
  urldate = {2019-05-16},
  keywords = {⛔ No DOI found,hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/QKMY8X8M/Schreiber_2018_pomegranate.pdf;/home/gdalle/snap/zotero-snap/common/Zotero/storage/9GWGT5RK/17-636.html}
}

@software{whiteJuliaDiffChainRulesJl2022a,
  title = {{{JuliaDiff}}/{{ChainRules}}.Jl: V1.23.0},
  shorttitle = {{{JuliaDiff}}/{{ChainRules}}.Jl},
  author = {White, Lyndon and Abbott, Michael and Zgubic, Miha and Revels, Jarrett and Arslan, Alex and Axen, Seth and Schaub, Simeon and Robinson, Nick and Ma, Yingbo and Dhingra, Gaurav and {willtebbutt} and Heim, Niklas and Widmann, David and Rosemberg, Andrew David Werner and Schmitz, Niklas and Rackauckas, Christopher and Heintzmann, Rainer and {frankschae} and Fischer, Keno and Robson, Alex and {mattBrzezinski} and Zhabinski, Andrei and Besançon, Mathieu and Vertechi, Pietro and Gowda, Shashi and Fitzgibbon, Andrew and Lucibello, Carlo and Vogt, Curtis and Gandhi, Dhairya and Chorney, Fernando},
  date = {2022-01-20},
  doi = {10.5281/zenodo.5881966},
  url = {https://zenodo.org/records/5881966},
  urldate = {2024-04-05},
  abstract = {ChainRules v1.23.0 Diff since v1.22.0 Merged pull requests: Use logten and logtwo from IrrationalConstants (\#555) (@devmotion)},
  organization = {Zenodo},
  version = {v1.23.0},
  keywords = {hmm},
  file = {/home/gdalle/snap/zotero-snap/common/Zotero/storage/BCB434HQ/5881966.html}
}
