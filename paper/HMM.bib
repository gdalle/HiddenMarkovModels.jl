@software{antonelloHMMGradientsJlEnables2021,
  title = {{{HMMGradients}}.Jl: {{Enables}} Computing the Gradient of the Parameters of {{Hidden Markov Models}} ({{HMMs}})},
  shorttitle = {Idiap/{{HMMGradients}}.Jl},
  author = {Antonello, Niccolò},
  date = {2021-06-07},
  doi = {10.5281/zenodo.4454565},
  url = {https://doi.org/10.5281/zenodo.4454565},
  urldate = {2023-09-12},
  organization = {{Zenodo}},
  keywords = {hmm},
  file = {/home/gdalle/Zotero/storage/PEFYSLF7/4906644.html}
}

@inproceedings{bengioInputOutputHMM1994,
  title = {An {{Input Output HMM Architecture}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Frasconi, Paolo},
  date = {1994},
  volume = {7},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1994/hash/8065d07da4a77621450aa84fee5656d9-Abstract.html},
  urldate = {2023-03-12},
  abstract = {We  introduce  a  recurrent  architecture  having  a  modular structure  and we formulate a training procedure based on the EM  algorithm.  The resulting model has similarities to hidden  Markov models, but  supports  recurrent  networks  processing style and allows  to exploit  the supervised  learning paradigm while using maximum likelihood  estimation.},
  keywords = {hmm,thesis},
  file = {/home/gdalle/Zotero/storage/68UNNYP2/Bengio_Frasconi_1994_An Input Output HMM Architecture.pdf}
}

@article{besanconDistributionsJlDefinition2021,
  title = {Distributions{{.jl}}: {{Definition}} and {{Modeling}} of {{Probability Distributions}} in the {{JuliaStats Ecosystem}}},
  shorttitle = {Distributions.Jl},
  author = {Besançon, Mathieu and Papamarkou, Theodore and Anthoff, David and Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  date = {2021-07-25},
  journaltitle = {Journal of Statistical Software},
  volume = {98},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v098.i16},
  url = {https://doi.org/10.18637/jss.v098.i16},
  urldate = {2022-09-19},
  abstract = {Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.},
  langid = {english},
  keywords = {hmm,thesis},
  file = {/home/gdalle/Zotero/storage/FZ5V2QNZ/Besancon et al_2021_Distributions.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  date = {2017-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  url = {https://epubs.siam.org/doi/10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  keywords = {hmm,inferopt,povar,thesis,viva},
  file = {/home/gdalle/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@book{cappeInferenceHiddenMarkov2005,
  title = {Inference in {{Hidden Markov Models}}},
  author = {Cappé, Olivier and Moulines, Eric and Rydén, Tobias},
  date = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  eprint = {4d_oEYn8Fl0C},
  eprinttype = {googlebooks},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/0-387-28982-8},
  url = {http://link.springer.com/10.1007/0-387-28982-8},
  urldate = {2022-12-03},
  isbn = {978-0-387-40264-2 978-0-387-28982-3},
  langid = {english},
  keywords = {hmm,povar,thesis},
  file = {/home/gdalle/Zotero/storage/2HYZE7ZD/Cappé et al_2005_Inference in Hidden Markov Models.pdf;/home/gdalle/Zotero/storage/QRNV9CL8/Cappé et al. - 2006 - Inference in Hidden Markov Models.pdf}
}

@thesis{dalleMachineLearningCombinatorial2022,
  type = {phdthesis},
  title = {Machine Learning and Combinatorial Optimization Algorithms, with Applications to Railway Planning},
  author = {Dalle, Guillaume},
  editora = {Meunier, Frédéric and De Castro, Yohann and Parmentier, Axel},
  editoratype = {collaborator},
  date = {2022-12-16},
  institution = {{École des Ponts ParisTech}},
  url = {https://www.theses.fr/2022ENPC0047},
  urldate = {2023-03-31},
  abbr = {Dissertation},
  abstract = {This thesis investigates the frontier between machine learning and combinatorial optimization, two active areas of applied mathematics research. We combine theoretical insights with efficient algorithms, and develop several open source Julia libraries. Inspired by a collaboration with the Société nationale des chemins de fer français (SNCF), we study high-impact use cases from the railway world: train failure prediction, delay propagation, and track allocation.In Part I, we provide mathematical background and describe software implementations for various tools that will be needed later on: implicit differentiation, temporal point processes, Hidden Markov Models and Multi-Agent Path Finding. Our publicly-available code fills a void in the Julia package ecosystem, aiming at ease of use without compromising on performance.In Part II, we highlight theoretical contributions related to both statistics and decision-making. We consider a Vector AutoRegressive process with partial observations, and prove matching upper and lower bounds on the estimation error. We unify and extend the state of the art for combinatorial optimization layers in deep learning, gathering various approaches in a Julia library called InferOpt.jl. We also seek to differentiate through multi-objective optimization layers, which leads to a novel theory of lexicographic convex analysis.In Part III, these mathematical and algorithmic foundations come together to tackle railway problems. We design a hierarchical model of train failures, propose a graph-based framework for delay propagation, and suggest new avenues for track allocation, with the Flatland challenge as a testing ground.},
  bibtex_show = {true},
  hal = {https://pastel.archives-ouvertes.fr/tel-04053322},
  langid = {english},
  selected = {true},
  keywords = {hmm,paper,website},
  file = {/home/gdalle/Zotero/storage/CEVJMUP4/Dalle - Machine learning and combinatorial optimization al.pdf}
}

@software{hmmlearnHmmlearnHiddenMarkov2023,
  title = {Hmmlearn: {{Hidden Markov Models}} in {{Python}}, with Scikit-Learn like {{API}}},
  author = {{hmmlearn}},
  date = {2023},
  url = {https://github.com/hmmlearn/hmmlearn},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models in Python, with scikit-learn like API},
  organization = {{hmmlearn}},
  keywords = {hmm}
}

@software{mouchetHMMBaseJlHidden2023,
  title = {{{HMMBase}}.Jl:  {{Hidden Markov Models}} for {{Julia}}},
  author = {Mouchet, Maxime},
  date = {2023},
  url = {https://github.com/maxmouchet/HMMBase.jl},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models for Julia.},
  keywords = {hmm}
}

@unpublished{ondelGPUAcceleratedForwardBackwardAlgorithm2021,
  title = {{{GPU-Accelerated Forward-Backward Algorithm}} with {{Application}} to {{Lattic-Free MMI}}},
  author = {Ondel, Lucas and Lam-Yee-Mui, Léa-Marie and Kocour, Martin and Filippo, Caio and Lukás Burget, Corro},
  date = {2021-11},
  url = {https://hal.science/hal-03434552},
  urldate = {2023-09-12},
  abstract = {We propose to express the forward-backward algorithm in terms of operations between sparse matrices in a specific semiring. This new perspective naturally leads to a GPU-friendly algorithm which is easy to implement in Julia or any programming languages with native support of semiring algebra. We use this new implementation to train a TDNN with the LF-MMI objective function and we compare the training time of our system with PyChain-a recently introduced C++/CUDA implementation of the LF-MMI loss. Our implementation is about two times faster while not having to use any approximation such as the "leaky-HMM".},
  keywords = {hmm},
  file = {/home/gdalle/Zotero/storage/XRKC5QBG/Ondel et al. - 2021 - GPU-Accelerated Forward-Backward Algorithm with Ap.pdf}
}

@article{qinDirectOptimizationApproach2000,
  title = {A {{Direct Optimization Approach}} to {{Hidden Markov Modeling}} for {{Single Channel Kinetics}}},
  author = {Qin, Feng and Auerbach, Anthony and Sachs, Frederick},
  date = {2000-10-01},
  journaltitle = {Biophysical Journal},
  shortjournal = {Biophysical Journal},
  volume = {79},
  number = {4},
  pages = {1915--1927},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(00)76441-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0006349500764411},
  urldate = {2022-08-06},
  abstract = {Hidden Markov modeling (HMM) provides an effective approach for modeling single channel kinetics. Standard HMM is based on Baum's reestimation. As applied to single channel currents, the algorithm has the inability to optimize the rate constants directly. We present here an alternative approach by considering the problem as a general optimization problem. The quasi-Newton method is used for searching the likelihood surface. The analytical derivatives of the likelihood function are derived, thereby maximizing the efficiency of the optimization. Because the rate constants are optimized directly, the approach has advantages such as the allowance for model constraints and the ability to simultaneously fit multiple data sets obtained at different experimental conditions. Numerical examples are presented to illustrate the performance of the algorithm. Comparisons with Baum's reestimation suggest that the approach has a superior convergence speed when the likelihood surface is poorly defined due to, for example, a low signal-to-noise ratio or the aggregation of multiple states having identical conductances.},
  langid = {english},
  keywords = {hmm,thesis,viva},
  file = {/home/gdalle/Zotero/storage/EPDNRHUX/Qin et al. - 2000 - A Direct Optimization Approach to Hidden Markov Mo.pdf;/home/gdalle/Zotero/storage/6C5WNKEU/S0006349500764411.html}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  date = {1989-02},
  journaltitle = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10/cswph2},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {done,hmm,thesis,viva},
  file = {/home/gdalle/Zotero/storage/A68ILRMJ/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/home/gdalle/Zotero/storage/BEJEKP4E/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/home/gdalle/Zotero/storage/5BHQF7ME/18626.html}
}

@software{rowleyLogarithmicNumbersJlLogarithmic2023,
  title = {{{LogarithmicNumbers}}.Jl: {{A}} Logarithmic Number System for {{Julia}}.},
  author = {Rowley, Christopher},
  date = {2023-05-24T15:06:29Z},
  url = {https://github.com/cjdoris/LogarithmicNumbers.jl},
  urldate = {2023-09-12},
  abstract = {A logarithmic number system for Julia.},
  keywords = {hmm}
}

@article{schreiberPomegranateFastFlexible2018a,
  title = {Pomegranate: {{Fast}} and {{Flexible Probabilistic Modeling}} in {{Python}}},
  shorttitle = {Pomegranate},
  author = {Schreiber, Jacob},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  number = {164},
  pages = {1--6},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-636.html},
  urldate = {2023-09-12},
  langid = {english},
  keywords = {⛔ No DOI found,hmm},
  file = {/home/gdalle/Zotero/storage/6DQMARYF/Schreiber - 2018 - pomegranate Fast and Flexible Probabilistic Model.pdf}
}

@software{whiteJuliaDiffChainRulesJl2022,
  title = {{{JuliaDiff}}/{{ChainRules}}{{.jl}}: V1.44.7},
  shorttitle = {{{JuliaDiff}}/{{ChainRules}}.Jl},
  author = {White, Frames Catherine and Abbott, Michael and Zgubic, Miha and Revels, Jarrett and Axen, Seth and Arslan, Alex and Schaub, Simeon and Robinson, Nick and Yingbo Ma and Gaurav Dhingra and Tebbutt, Will and Heim, Niklas and Widmann, David and Rosemberg, Andrew David Werner and Schmitz, Niklas and Rackauckas, Christopher and Heintzmann, Rainer and Frankschae and Noack, Andreas and Lucibello, Carlo and Fischer, Keno and Robson, Alex and Cossio and Ling, Jerry and MattBrzezinski and Finnegan, Rory and Zhabinski, Andrei and Wennberg, Daniel and Besançon, Mathieu and Vertechi, Pietro},
  date = {2022-10-10},
  doi = {10.5281/ZENODO.4754896},
  url = {https://zenodo.org/record/4754896},
  urldate = {2022-10-13},
  abstract = {ChainRules v1.44.7 Diff since v1.44.6 {$<$}strong{$>$}Closed issues:{$<$}/strong{$>$} cat with Val tuple dims fails (\#678) {$<$}strong{$>$}Merged pull requests:{$<$}/strong{$>$} Fix for ChainRulesCore \#586 (\#675) (@rofinn) fix cat rrule (\#679) (@cossio)},
  organization = {{Zenodo}},
  version = {v1.44.7},
  keywords = {\#nosource,hmm,inferopt,thesis}
}
